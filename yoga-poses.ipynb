{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1559111,"sourceType":"datasetVersion","datasetId":920599},{"sourceId":2083498,"sourceType":"datasetVersion","datasetId":1249230}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install opencv-python mediapipe numpy tensorflow\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:16.060903Z","iopub.execute_input":"2024-12-10T19:57:16.061268Z","iopub.status.idle":"2024-12-10T19:57:28.440348Z","shell.execute_reply.started":"2024-12-10T19:57:16.061232Z","shell.execute_reply":"2024-12-10T19:57:28.439249Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (4.10.0.84)\nCollecting mediapipe\n  Downloading mediapipe-0.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.16.1)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mediapipe) (1.4.0)\nRequirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (23.2.0)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (24.3.25)\nRequirement already satisfied: jax in /opt/conda/lib/python3.10/site-packages (from mediapipe) (0.4.26)\nRequirement already satisfied: jaxlib in /opt/conda/lib/python3.10/site-packages (from mediapipe) (0.4.26.dev20240620)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mediapipe) (3.7.5)\nRequirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.10/site-packages (from mediapipe) (4.10.0.84)\nCollecting protobuf<5,>=4.25.3 (from mediapipe)\n  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nCollecting sounddevice>=0.4.4 (from mediapipe)\n  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from mediapipe) (0.2.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.11.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.3.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (70.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.62.2)\nRequirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.16.2)\nRequirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.3)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.37.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\nRequirement already satisfied: CFFI>=1.0 in /opt/conda/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax->mediapipe) (1.14.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\nDownloading mediapipe-0.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.1/36.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\nInstalling collected packages: protobuf, sounddevice, mediapipe\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.1.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.5 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.5 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\nkfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed mediapipe-0.10.18 protobuf-4.25.3 sounddevice-0.5.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport cv2\nimport mediapipe as mp\nimport pandas as pd\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:28.442190Z","iopub.execute_input":"2024-12-10T19:57:28.442505Z","iopub.status.idle":"2024-12-10T19:57:39.662400Z","shell.execute_reply.started":"2024-12-10T19:57:28.442476Z","shell.execute_reply":"2024-12-10T19:57:39.661695Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"mp_pose = mp.solutions.pose\npose = mp_pose.Pose()\nmp_drawing = mp.solutions.drawing_utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:39.663369Z","iopub.execute_input":"2024-12-10T19:57:39.663803Z","iopub.status.idle":"2024-12-10T19:57:39.677418Z","shell.execute_reply.started":"2024-12-10T19:57:39.663777Z","shell.execute_reply":"2024-12-10T19:57:39.676428Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"DATASET_PATH = \"/kaggle/input/yoga-poses-dataset/DATASET/TRAIN/\"  # Replace with your dataset folder path\nPOSE_LABELS = [\"downdog\", \"goddess\", \"plank\", \"tree\", \"warrior2\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:01:14.482363Z","iopub.execute_input":"2024-12-10T20:01:14.483189Z","iopub.status.idle":"2024-12-10T20:01:14.487551Z","shell.execute_reply.started":"2024-12-10T20:01:14.483153Z","shell.execute_reply":"2024-12-10T20:01:14.486619Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"data = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:01:17.209367Z","iopub.execute_input":"2024-12-10T20:01:17.209711Z","iopub.status.idle":"2024-12-10T20:01:17.213553Z","shell.execute_reply.started":"2024-12-10T20:01:17.209682Z","shell.execute_reply":"2024-12-10T20:01:17.212723Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"for pose_label in POSE_LABELS:\n    pose_dir = os.path.join(DATASET_PATH, pose_label)\n    for img_file in os.listdir(pose_dir):\n        img_path = os.path.join(pose_dir, img_file)\n\n        # Read the image\n        image = cv2.imread(img_path)\n        if image is None:\n            print(f\"Failed to read {img_path}\")\n            continue\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Process the image with Mediapipe\n        results = pose.process(image_rgb)\n\n        if results.pose_landmarks:\n            # Extract keypoints\n            landmarks = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark]).flatten()\n            data.append([pose_label] + landmarks.tolist())\n        else:\n            print(f\"No landmarks detected for {img_file} {pose_label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:01:19.044498Z","iopub.execute_input":"2024-12-10T20:01:19.044851Z","iopub.status.idle":"2024-12-10T20:02:10.177037Z","shell.execute_reply.started":"2024-12-10T20:01:19.044822Z","shell.execute_reply":"2024-12-10T20:02:10.176256Z"}},"outputs":[{"name":"stderr","text":"W0000 00:00:1733860879.153628      97 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n","output_type":"stream"},{"name":"stdout","text":"No landmarks detected for 00000387.jpg downdog\nNo landmarks detected for 00000147.jpg downdog\nNo landmarks detected for 00000270.jpg downdog\nNo landmarks detected for 00000128.jpg downdog\nNo landmarks detected for 00000315.jpg downdog\nNo landmarks detected for 00000220.jpg downdog\nNo landmarks detected for 00000345.png downdog\nNo landmarks detected for 00000255.jpg downdog\nNo landmarks detected for 00000217.jpg downdog\nNo landmarks detected for 00000183.jpg downdog\nNo landmarks detected for 00000235.jpg downdog\nNo landmarks detected for 00000320.jpg downdog\nNo landmarks detected for 00000306.jpg downdog\nNo landmarks detected for 00000340.jpg downdog\nNo landmarks detected for 00000188.png downdog\nNo landmarks detected for 00000266.jpg downdog\nNo landmarks detected for 00000263.jpg downdog\nNo landmarks detected for 00000212.png downdog\nNo landmarks detected for 00000274.jpg downdog\nNo landmarks detected for 00000168.jpg downdog\nNo landmarks detected for 00000385.png downdog\nNo landmarks detected for 00000316.jpg downdog\nNo landmarks detected for 00000367.png downdog\nNo landmarks detected for 00000286.jpg downdog\nNo landmarks detected for 00000194.jpg downdog\nNo landmarks detected for 00000232.jpg downdog\nNo landmarks detected for 00000222.jpg downdog\nNo landmarks detected for 00000115.jpg goddess\nNo landmarks detected for 00000211.jpg goddess\nNo landmarks detected for 00000141.jpg goddess\nNo landmarks detected for 00000204.jpg goddess\nNo landmarks detected for 00000100.jpg goddess\nNo landmarks detected for 00000315.jpg goddess\nNo landmarks detected for 00000299.jpg goddess\nNo landmarks detected for 00000281.jpg goddess\nNo landmarks detected for 00000202.jpg goddess\nNo landmarks detected for 00000320.jpg goddess\nNo landmarks detected for 00000127.png goddess\nNo landmarks detected for 00000280.jpg goddess\nNo landmarks detected for 00000173.jpg goddess\nNo landmarks detected for 00000191.jpg goddess\nNo landmarks detected for 00000111.jpg goddess\nNo landmarks detected for 00000329.jpg goddess\nNo landmarks detected for 00000149.jpg goddess\nNo landmarks detected for 00000282.png goddess\nNo landmarks detected for 00000317.jpg goddess\nNo landmarks detected for 00000146.jpg goddess\n","output_type":"stream"},{"name":"stderr","text":"Corrupt JPEG data: premature end of data segment\n","output_type":"stream"},{"name":"stdout","text":"No landmarks detected for 00000437.jpg plank\nNo landmarks detected for 00000284.jpg plank\nNo landmarks detected for 00000311.jpg plank\nNo landmarks detected for 00000307.png plank\nNo landmarks detected for 00000207.jpg plank\nNo landmarks detected for 00000420.jpg plank\nNo landmarks detected for 00000380.jpg plank\nNo landmarks detected for 00000240.jpg plank\nNo landmarks detected for 00000387.jpeg plank\nNo landmarks detected for 00000306.jpg plank\nNo landmarks detected for 00000373.jpg plank\nNo landmarks detected for 00000134.jpg plank\nNo landmarks detected for 00000191.jpg plank\nNo landmarks detected for 00000229.jpg plank\nNo landmarks detected for 00000236.png plank\nNo landmarks detected for 00000266.jpg plank\nNo landmarks detected for 00000244.jpg plank\nNo landmarks detected for 00000210.jpg plank\nNo landmarks detected for 00000305.jpg plank\nNo landmarks detected for 00000319.jpg plank\nNo landmarks detected for 00000386.jpg plank\nNo landmarks detected for 00000269.jpg plank\nNo landmarks detected for 00000157.jpg plank\nNo landmarks detected for 00000186.jpg plank\nNo landmarks detected for 00000189.jpg plank\nNo landmarks detected for 00000171.jpg plank\nNo landmarks detected for 00000374.jpg plank\nNo landmarks detected for 00000295.jpg plank\nNo landmarks detected for 00000345.jpg plank\nNo landmarks detected for 00000275.jpg plank\nNo landmarks detected for 00000435.JPG plank\nNo landmarks detected for File10.jpg tree\nNo landmarks detected for File83.jpg tree\nNo landmarks detected for 00000126.jpg tree\n","output_type":"stream"},{"name":"stderr","text":"Premature end of JPEG file\n","output_type":"stream"},{"name":"stdout","text":"No landmarks detected for 00000202.jpg tree\nNo landmarks detected for File41.jpg tree\nNo landmarks detected for 00000134.jpg tree\nNo landmarks detected for 00000143.jpg tree\nNo landmarks detected for 00000153.jpg tree\nNo landmarks detected for 00000177.png tree\nNo landmarks detected for 00000106.jpg tree\nNo landmarks detected for 00000159.jpg tree\nNo landmarks detected for 00000158.jpg tree\nNo landmarks detected for 00000186.jpg tree\nNo landmarks detected for 00000121.jpg tree\nNo landmarks detected for 00000138.jpg tree\nNo landmarks detected for File36.jpg tree\nNo landmarks detected for 00000078.jpg tree\nNo landmarks detected for 00000150.jpg tree\nNo landmarks detected for 00000146.jpg tree\nNo landmarks detected for 00000166.jpg tree\nNo landmarks detected for 00000151.jpg tree\nNo landmarks detected for 00000309.jpg warrior2\nNo landmarks detected for 00000326.jpg warrior2\nNo landmarks detected for 00000141.jpg warrior2\nNo landmarks detected for 00000359.jpg warrior2\nNo landmarks detected for 00000299.jpg warrior2\nNo landmarks detected for 00000342.jpg warrior2\nNo landmarks detected for 00000355.jpg warrior2\nNo landmarks detected for 00000352.jpg warrior2\nNo landmarks detected for 00000338.jpg warrior2\nNo landmarks detected for 00000344.jpg warrior2\nNo landmarks detected for 00000237.jpg warrior2\nNo landmarks detected for 00000423.jpg warrior2\nNo landmarks detected for 00000349.jpg warrior2\nNo landmarks detected for 00000226.jpg warrior2\nNo landmarks detected for 00000121.jpg warrior2\nNo landmarks detected for 00000140.jpg warrior2\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(len(data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:02:15.860181Z","iopub.execute_input":"2024-12-10T20:02:15.860974Z","iopub.status.idle":"2024-12-10T20:02:15.865442Z","shell.execute_reply.started":"2024-12-10T20:02:15.860939Z","shell.execute_reply":"2024-12-10T20:02:15.864533Z"}},"outputs":[{"name":"stdout","text":"966\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"columns = [\"label\"] + [f\"kp_{i}\" for i in range(len(data[0]) - 1)]\ndf = pd.DataFrame(data, columns=columns)\ndf.to_csv(\"/kaggle/working/yoga_pose_dataset.csv\", index=False)\nprint(\"Keypoints dataset saved as yoga_pose_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:02:24.179364Z","iopub.execute_input":"2024-12-10T20:02:24.180027Z","iopub.status.idle":"2024-12-10T20:02:24.382408Z","shell.execute_reply.started":"2024-12-10T20:02:24.179996Z","shell.execute_reply":"2024-12-10T20:02:24.381523Z"}},"outputs":[{"name":"stdout","text":"Keypoints dataset saved as yoga_pose_dataset.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/working/yoga_pose_dataset.csv\")\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:02:28.807993Z","iopub.execute_input":"2024-12-10T20:02:28.808301Z","iopub.status.idle":"2024-12-10T20:02:28.867247Z","shell.execute_reply.started":"2024-12-10T20:02:28.808275Z","shell.execute_reply":"2024-12-10T20:02:28.866383Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"        label      kp_0      kp_1      kp_2      kp_3      kp_4      kp_5  \\\n0     downdog  0.475104  0.719103 -0.047471  0.999659  0.459097  0.727051   \n1     downdog  0.343626  0.600951 -0.394967  0.998327  0.348699  0.607715   \n2     downdog  0.421144  0.709083 -1.644430  0.997462  0.393961  0.665963   \n3     downdog  0.157875  0.704797 -0.076791  0.948053  0.183123  0.708136   \n4     downdog  0.429942  0.539722 -0.098656  0.946747  0.407345  0.546577   \n..        ...       ...       ...       ...       ...       ...       ...   \n961  warrior2  0.497345  0.117137 -0.721816  0.992677  0.509624  0.095908   \n962  warrior2  0.466859  0.375286 -0.792588  0.993399  0.470250  0.363588   \n963  warrior2  0.352991  0.110771 -0.308266  0.994027  0.353158  0.095806   \n964  warrior2  0.547110  0.279410 -0.207243  0.992033  0.532679  0.268209   \n965  warrior2  0.472693  0.086808 -0.243353  0.992693  0.487475  0.073602   \n\n         kp_6      kp_7      kp_8  ...    kp_122    kp_123    kp_124  \\\n0   -0.075667  0.999786  0.456559  ...  0.342115  0.633482  0.753560   \n1   -0.393844  0.998779  0.349357  ...  0.167515  0.656265  0.696353   \n2   -1.687572  0.998338  0.387795  ...  1.138510  0.618183  0.779101   \n3   -0.067712  0.954240  0.184699  ... -0.047511  0.647820  0.757851   \n4   -0.095738  0.955199  0.404468  ...  0.272630  0.655964  1.401836   \n..        ...       ...       ...  ...       ...       ...       ...   \n961 -0.690161  0.993827  0.515670  ...  0.079718  0.881002  0.895122   \n962 -0.771894  0.994432  0.471571  ...  0.625432  0.886684  0.045259   \n963 -0.256537  0.994941  0.355457  ...  0.126599  0.890285  0.565148   \n964 -0.177967  0.993205  0.531282  ...  0.043850  0.898914  0.705270   \n965 -0.231112  0.993679  0.491659  ...  0.128531  0.896856  0.722871   \n\n       kp_125    kp_126    kp_127    kp_128    kp_129    kp_130    kp_131  \n0    0.800184 -0.074128  0.983391  0.734868  0.792172  0.278917  0.693638  \n1    0.596452  0.278771  0.973590  0.287103  0.782898  0.138191  0.705881  \n2    0.840317  1.205693  0.911469  0.742237  0.874636  1.183832  0.669940  \n3    0.761291  0.350289  0.874858  0.814097  0.747398  0.042645  0.688334  \n4    0.263161  0.542816  0.819312  1.400406  0.203339  0.214396  0.659724  \n..        ...       ...       ...       ...       ...       ...       ...  \n961  0.964375 -0.371914  0.908138  0.164844  0.845071 -0.122726  0.886003  \n962  0.327536 -0.027923  0.913494  0.404698  0.411615  0.625475  0.886186  \n963  0.969692 -0.206443  0.917645  0.119440  0.956569  0.020155  0.894757  \n964  0.857454  0.421258  0.917262  0.251259  0.944915 -0.091032  0.902524  \n965  1.052852  0.213765  0.918864  0.289270  1.015739  0.010229  0.904301  \n\n[966 rows x 133 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>kp_0</th>\n      <th>kp_1</th>\n      <th>kp_2</th>\n      <th>kp_3</th>\n      <th>kp_4</th>\n      <th>kp_5</th>\n      <th>kp_6</th>\n      <th>kp_7</th>\n      <th>kp_8</th>\n      <th>...</th>\n      <th>kp_122</th>\n      <th>kp_123</th>\n      <th>kp_124</th>\n      <th>kp_125</th>\n      <th>kp_126</th>\n      <th>kp_127</th>\n      <th>kp_128</th>\n      <th>kp_129</th>\n      <th>kp_130</th>\n      <th>kp_131</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>downdog</td>\n      <td>0.475104</td>\n      <td>0.719103</td>\n      <td>-0.047471</td>\n      <td>0.999659</td>\n      <td>0.459097</td>\n      <td>0.727051</td>\n      <td>-0.075667</td>\n      <td>0.999786</td>\n      <td>0.456559</td>\n      <td>...</td>\n      <td>0.342115</td>\n      <td>0.633482</td>\n      <td>0.753560</td>\n      <td>0.800184</td>\n      <td>-0.074128</td>\n      <td>0.983391</td>\n      <td>0.734868</td>\n      <td>0.792172</td>\n      <td>0.278917</td>\n      <td>0.693638</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>downdog</td>\n      <td>0.343626</td>\n      <td>0.600951</td>\n      <td>-0.394967</td>\n      <td>0.998327</td>\n      <td>0.348699</td>\n      <td>0.607715</td>\n      <td>-0.393844</td>\n      <td>0.998779</td>\n      <td>0.349357</td>\n      <td>...</td>\n      <td>0.167515</td>\n      <td>0.656265</td>\n      <td>0.696353</td>\n      <td>0.596452</td>\n      <td>0.278771</td>\n      <td>0.973590</td>\n      <td>0.287103</td>\n      <td>0.782898</td>\n      <td>0.138191</td>\n      <td>0.705881</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>downdog</td>\n      <td>0.421144</td>\n      <td>0.709083</td>\n      <td>-1.644430</td>\n      <td>0.997462</td>\n      <td>0.393961</td>\n      <td>0.665963</td>\n      <td>-1.687572</td>\n      <td>0.998338</td>\n      <td>0.387795</td>\n      <td>...</td>\n      <td>1.138510</td>\n      <td>0.618183</td>\n      <td>0.779101</td>\n      <td>0.840317</td>\n      <td>1.205693</td>\n      <td>0.911469</td>\n      <td>0.742237</td>\n      <td>0.874636</td>\n      <td>1.183832</td>\n      <td>0.669940</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>downdog</td>\n      <td>0.157875</td>\n      <td>0.704797</td>\n      <td>-0.076791</td>\n      <td>0.948053</td>\n      <td>0.183123</td>\n      <td>0.708136</td>\n      <td>-0.067712</td>\n      <td>0.954240</td>\n      <td>0.184699</td>\n      <td>...</td>\n      <td>-0.047511</td>\n      <td>0.647820</td>\n      <td>0.757851</td>\n      <td>0.761291</td>\n      <td>0.350289</td>\n      <td>0.874858</td>\n      <td>0.814097</td>\n      <td>0.747398</td>\n      <td>0.042645</td>\n      <td>0.688334</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>downdog</td>\n      <td>0.429942</td>\n      <td>0.539722</td>\n      <td>-0.098656</td>\n      <td>0.946747</td>\n      <td>0.407345</td>\n      <td>0.546577</td>\n      <td>-0.095738</td>\n      <td>0.955199</td>\n      <td>0.404468</td>\n      <td>...</td>\n      <td>0.272630</td>\n      <td>0.655964</td>\n      <td>1.401836</td>\n      <td>0.263161</td>\n      <td>0.542816</td>\n      <td>0.819312</td>\n      <td>1.400406</td>\n      <td>0.203339</td>\n      <td>0.214396</td>\n      <td>0.659724</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>961</th>\n      <td>warrior2</td>\n      <td>0.497345</td>\n      <td>0.117137</td>\n      <td>-0.721816</td>\n      <td>0.992677</td>\n      <td>0.509624</td>\n      <td>0.095908</td>\n      <td>-0.690161</td>\n      <td>0.993827</td>\n      <td>0.515670</td>\n      <td>...</td>\n      <td>0.079718</td>\n      <td>0.881002</td>\n      <td>0.895122</td>\n      <td>0.964375</td>\n      <td>-0.371914</td>\n      <td>0.908138</td>\n      <td>0.164844</td>\n      <td>0.845071</td>\n      <td>-0.122726</td>\n      <td>0.886003</td>\n    </tr>\n    <tr>\n      <th>962</th>\n      <td>warrior2</td>\n      <td>0.466859</td>\n      <td>0.375286</td>\n      <td>-0.792588</td>\n      <td>0.993399</td>\n      <td>0.470250</td>\n      <td>0.363588</td>\n      <td>-0.771894</td>\n      <td>0.994432</td>\n      <td>0.471571</td>\n      <td>...</td>\n      <td>0.625432</td>\n      <td>0.886684</td>\n      <td>0.045259</td>\n      <td>0.327536</td>\n      <td>-0.027923</td>\n      <td>0.913494</td>\n      <td>0.404698</td>\n      <td>0.411615</td>\n      <td>0.625475</td>\n      <td>0.886186</td>\n    </tr>\n    <tr>\n      <th>963</th>\n      <td>warrior2</td>\n      <td>0.352991</td>\n      <td>0.110771</td>\n      <td>-0.308266</td>\n      <td>0.994027</td>\n      <td>0.353158</td>\n      <td>0.095806</td>\n      <td>-0.256537</td>\n      <td>0.994941</td>\n      <td>0.355457</td>\n      <td>...</td>\n      <td>0.126599</td>\n      <td>0.890285</td>\n      <td>0.565148</td>\n      <td>0.969692</td>\n      <td>-0.206443</td>\n      <td>0.917645</td>\n      <td>0.119440</td>\n      <td>0.956569</td>\n      <td>0.020155</td>\n      <td>0.894757</td>\n    </tr>\n    <tr>\n      <th>964</th>\n      <td>warrior2</td>\n      <td>0.547110</td>\n      <td>0.279410</td>\n      <td>-0.207243</td>\n      <td>0.992033</td>\n      <td>0.532679</td>\n      <td>0.268209</td>\n      <td>-0.177967</td>\n      <td>0.993205</td>\n      <td>0.531282</td>\n      <td>...</td>\n      <td>0.043850</td>\n      <td>0.898914</td>\n      <td>0.705270</td>\n      <td>0.857454</td>\n      <td>0.421258</td>\n      <td>0.917262</td>\n      <td>0.251259</td>\n      <td>0.944915</td>\n      <td>-0.091032</td>\n      <td>0.902524</td>\n    </tr>\n    <tr>\n      <th>965</th>\n      <td>warrior2</td>\n      <td>0.472693</td>\n      <td>0.086808</td>\n      <td>-0.243353</td>\n      <td>0.992693</td>\n      <td>0.487475</td>\n      <td>0.073602</td>\n      <td>-0.231112</td>\n      <td>0.993679</td>\n      <td>0.491659</td>\n      <td>...</td>\n      <td>0.128531</td>\n      <td>0.896856</td>\n      <td>0.722871</td>\n      <td>1.052852</td>\n      <td>0.213765</td>\n      <td>0.918864</td>\n      <td>0.289270</td>\n      <td>1.015739</td>\n      <td>0.010229</td>\n      <td>0.904301</td>\n    </tr>\n  </tbody>\n</table>\n<p>966 rows × 133 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/working/yoga_pose_dataset.csv\")\n\n# Drop the label column to isolate the 132 points\npoints = df.drop(\"label\", axis=1)\n\n# Calculate the average of all 132 points grouped by labels\naverage_points_per_label = df.groupby(\"label\").mean()\n\n# Display the result\nprint(average_points_per_label)\n\n# Save the averages to a CSV file (optional)\naverage_points_per_label.to_csv(\"/kaggle/working/yoga_pose_averages.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:02:32.012522Z","iopub.execute_input":"2024-12-10T20:02:32.012909Z","iopub.status.idle":"2024-12-10T20:02:32.067314Z","shell.execute_reply.started":"2024-12-10T20:02:32.012865Z","shell.execute_reply":"2024-12-10T20:02:32.066573Z"}},"outputs":[{"name":"stdout","text":"              kp_0      kp_1      kp_2      kp_3      kp_4      kp_5  \\\nlabel                                                                  \ndowndog   0.449599  0.624624 -0.175095  0.959825  0.445584  0.627712   \ngoddess   0.508224  0.313755 -0.289135  0.952116  0.515464  0.294410   \nplank     0.521016  0.511434 -0.241952  0.974537  0.523263  0.503317   \ntree      0.510776  0.263988 -0.367704  0.956248  0.520069  0.247247   \nwarrior2  0.485951  0.308342 -0.216253  0.933172  0.489614  0.291592   \n\n              kp_6      kp_7      kp_8      kp_9  ...    kp_122    kp_123  \\\nlabel                                             ...                       \ndowndog  -0.190877  0.963792  0.445654  0.627578  ...  0.270495  0.676458   \ngoddess  -0.263825  0.958273  0.520362  0.294754  ...  0.175683  0.758876   \nplank    -0.248244  0.974990  0.523655  0.502194  ...  0.156558  0.796288   \ntree     -0.347004  0.955262  0.525933  0.248773  ...  0.283706  0.635121   \nwarrior2 -0.216982  0.939963  0.492696  0.291971  ...  0.202764  0.783511   \n\n            kp_124    kp_125    kp_126    kp_127    kp_128    kp_129  \\\nlabel                                                                  \ndowndog   0.599349  0.741756  0.133106  0.784255  0.578271  0.734098   \ngoddess   0.632097  1.022060  0.052263  0.758251  0.372375  1.020022   \nplank     0.463499  0.731207  0.117077  0.806230  0.469768  0.729547   \ntree      0.511074  0.916598  0.094083  0.680975  0.510160  0.876932   \nwarrior2  0.607313  0.944280  0.116287  0.782478  0.378737  0.941708   \n\n            kp_130    kp_131  \nlabel                         \ndowndog   0.187643  0.692113  \ngoddess   0.047326  0.743078  \nplank     0.072652  0.804876  \ntree      0.188062  0.631995  \nwarrior2  0.108605  0.773144  \n\n[5 rows x 132 columns]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/working/yoga_pose_dataset.csv\")\n\n# Group by the label and calculate the mean of the landmarks\naverage_landmarks = df.groupby(\"label\").mean()\n\n# Convert the grouped averages into the `ideal_poses` dictionary\nideal_poses = {}\nfor label in average_landmarks.index:\n    ideal_poses[label] = average_landmarks.loc[label].values\n\n# Display the ideal_poses dictionary\nprint(ideal_poses)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:03:38.187921Z","iopub.execute_input":"2024-12-10T20:03:38.188609Z","iopub.status.idle":"2024-12-10T20:03:38.225045Z","shell.execute_reply.started":"2024-12-10T20:03:38.188577Z","shell.execute_reply":"2024-12-10T20:03:38.224218Z"}},"outputs":[{"name":"stdout","text":"{'downdog': array([ 0.44959943,  0.62462382, -0.17509497,  0.95982483,  0.44558357,\n        0.62771218, -0.19087664,  0.96379154,  0.44565449,  0.62757769,\n       -0.19087752,  0.9671416 ,  0.44488938,  0.62679244, -0.19092047,\n        0.96662396,  0.44473306,  0.62937687, -0.18169059,  0.96517282,\n        0.44411915,  0.62943019, -0.18152041,  0.96534933,  0.44360461,\n        0.62894313, -0.18170592,  0.96251108,  0.44705243,  0.60555018,\n       -0.1855737 ,  0.96545968,  0.44375708,  0.60717282, -0.15849334,\n        0.96091721,  0.45269052,  0.60764933, -0.16554656,  0.96748934,\n        0.45294945,  0.60702908, -0.1560382 ,  0.96578179,  0.45338822,\n        0.53968365, -0.16039762,  0.98215709,  0.44931642,  0.53931335,\n       -0.09072029,  0.98000823,  0.43276744,  0.64010334, -0.20610748,\n        0.6739397 ,  0.42858456,  0.64834929, -0.05923584,  0.52457449,\n        0.40417272,  0.76666744, -0.24299717,  0.75618871,  0.39606883,\n        0.76901708, -0.11324811,  0.68176067,  0.40089008,  0.79036582,\n       -0.27612522,  0.76045935,  0.38622369,  0.78803102, -0.13147145,\n        0.71854707,  0.3931214 ,  0.792877  , -0.26758782,  0.7590763 ,\n        0.38913095,  0.79134782, -0.15019258,  0.72196855,  0.39553368,\n        0.78624151, -0.23975033,  0.74772334,  0.39190069,  0.78488643,\n       -0.12284457,  0.71088151,  0.54339533,  0.28962894, -0.02589796,\n        0.95935206,  0.53436839,  0.29363196,  0.02590216,  0.95764138,\n        0.57185957,  0.47601099,  0.06232954,  0.73408854,  0.54984605,\n        0.49028781,  0.10807775,  0.57106616,  0.60639506,  0.66755505,\n        0.21796251,  0.76385512,  0.57934065,  0.6665411 ,  0.25692951,\n        0.63848514,  0.61428491,  0.69437227,  0.23016474,  0.76869825,\n        0.58551387,  0.68529754,  0.27049474,  0.6764579 ,  0.59934932,\n        0.74175626,  0.13310626,  0.78425497,  0.5782709 ,  0.7340978 ,\n        0.18764278,  0.69211254]), 'goddess': array([ 0.50822368,  0.3137547 , -0.28913458,  0.95211632,  0.51546397,\n        0.29440993, -0.2638248 ,  0.95827284,  0.52036204,  0.29475393,\n       -0.26471878,  0.96064936,  0.52493938,  0.2942279 , -0.26688617,\n        0.96031533,  0.5020831 ,  0.29550417, -0.26786793,  0.95576595,\n        0.49767751,  0.29640324, -0.26776129,  0.9543711 ,  0.49316596,\n        0.29694612, -0.26672249,  0.95283667,  0.53068629,  0.30508285,\n       -0.15587249,  0.96067086,  0.4871391 ,  0.30745698, -0.15005072,\n        0.95626228,  0.51776205,  0.33395212, -0.23996933,  0.96083268,\n        0.50052768,  0.33588099, -0.24125567,  0.95929753,  0.57134109,\n        0.40500156, -0.09952764,  0.97174621,  0.44564563,  0.41073681,\n       -0.08895564,  0.96674024,  0.61815466,  0.4664047 , -0.16289142,\n        0.82921093,  0.38712295,  0.46786821, -0.16004554,  0.78111667,\n        0.60571363,  0.39210491, -0.27687955,  0.83386505,  0.39567266,\n        0.40730464, -0.27946048,  0.80276043,  0.60509459,  0.37134888,\n       -0.3194692 ,  0.80178472,  0.39673598,  0.38812225, -0.31654729,\n        0.78000669,  0.60108095,  0.35898071, -0.3120428 ,  0.80092785,\n        0.40047881,  0.37818163, -0.31510185,  0.77921366,  0.59810009,\n        0.36420629, -0.27922249,  0.79002609,  0.40089523,  0.38492468,\n       -0.28462467,  0.77182783,  0.55054842,  0.71612574, -0.00989408,\n        0.93432291,  0.46680886,  0.71803566,  0.00997891,  0.93533166,\n        0.63238354,  0.77450742, -0.17437398,  0.83930904,  0.37062203,\n        0.78506129, -0.1349396 ,  0.81033985,  0.6112914 ,  0.95882607,\n        0.15116356,  0.78244975,  0.39872257,  0.96265365,  0.1509558 ,\n        0.76783299,  0.59885959,  0.98889962,  0.17605815,  0.76566326,\n        0.41107211,  0.98764541,  0.17568346,  0.75887573,  0.63209712,\n        1.0220601 ,  0.052263  ,  0.75825054,  0.37237474,  1.02002199,\n        0.0473265 ,  0.74307774]), 'plank': array([ 0.52101556,  0.51143367, -0.2419524 ,  0.97453716,  0.52326274,\n        0.50331707, -0.24824386,  0.97498955,  0.52365456,  0.50219375,\n       -0.24831865,  0.97768543,  0.52413184,  0.50115171, -0.2484226 ,\n        0.97523192,  0.52139282,  0.50605817, -0.24975231,  0.97629708,\n        0.52126702,  0.50555184, -0.24975416,  0.97655184,  0.5206217 ,\n        0.5062277 , -0.24998476,  0.9725872 ,  0.52369834,  0.49547325,\n       -0.22539733,  0.97535148,  0.5178787 ,  0.50315683, -0.23233914,\n        0.97258861,  0.51966267,  0.5135038 , -0.22125203,  0.9772878 ,\n        0.5182395 ,  0.51471966, -0.22698186,  0.97423985,  0.52740908,\n        0.50943686, -0.17326198,  0.99061394,  0.50611248,  0.51997881,\n       -0.17743248,  0.98992119,  0.53364337,  0.57686209, -0.18645049,\n        0.69290194,  0.49466274,  0.59996224, -0.19307319,  0.71540025,\n        0.54295937,  0.63714422, -0.26705344,  0.7723129 ,  0.49267465,\n        0.68600945, -0.23762133,  0.76413368,  0.5459111 ,  0.6478406 ,\n       -0.29971462,  0.76352196,  0.49085907,  0.69656084, -0.26226657,\n        0.7556277 ,  0.54277395,  0.64705858, -0.3141425 ,  0.76581623,\n        0.49209446,  0.69243386, -0.28003104,  0.75575389,  0.54250122,\n        0.64341318, -0.27733553,  0.76026157,  0.49265993,  0.68773028,\n       -0.24781562,  0.75268758,  0.49823434,  0.57050699,  0.00448264,\n        0.98421415,  0.48062705,  0.57647708, -0.00456743,  0.98599836,\n        0.49554204,  0.62157615,  0.05881461,  0.76593468,  0.47796123,\n        0.62090551,  0.03665443,  0.79052268,  0.47192466,  0.69498156,\n        0.19037207,  0.7912374 ,  0.47407712,  0.69491736,  0.14721462,\n        0.79568813,  0.47069248,  0.7028279 ,  0.20169996,  0.79437565,\n        0.47459661,  0.70178363,  0.15655849,  0.79628826,  0.46349931,\n        0.73120747,  0.11707734,  0.80623028,  0.46976769,  0.72954747,\n        0.07265242,  0.80487594]), 'tree': array([ 0.51077634,  0.26398807, -0.36770376,  0.95624769,  0.52006895,\n        0.2472468 , -0.34700418,  0.95526158,  0.52593256,  0.24877305,\n       -0.34702016,  0.9583285 ,  0.53053254,  0.24866765, -0.34716612,\n        0.95731834,  0.50110524,  0.24686803, -0.34721129,  0.95310434,\n        0.49541016,  0.24735171, -0.34714571,  0.95238759,  0.48901583,\n        0.24893374, -0.3475275 ,  0.95025047,  0.53730912,  0.25674184,\n       -0.1960633 ,  0.95520942,  0.47937312,  0.25723734, -0.1895715 ,\n        0.94972518,  0.52253065,  0.28613704, -0.29915861,  0.94974814,\n        0.49977502,  0.28718037, -0.30313983,  0.94423181,  0.57712212,\n        0.35435963, -0.1516999 ,  0.96015788,  0.43522318,  0.35709681,\n       -0.1226009 ,  0.96063635,  0.60026427,  0.35169874, -0.27775455,\n        0.79273883,  0.40122675,  0.36337314, -0.21950348,  0.77771293,\n        0.55270616,  0.28604099, -0.40097271,  0.77028446,  0.44466507,\n        0.28307865, -0.33885469,  0.76282867,  0.54007458,  0.26181077,\n       -0.44815883,  0.72889828,  0.45356409,  0.26444478, -0.38081744,\n        0.72019714,  0.53926061,  0.25487239, -0.44047432,  0.72368455,\n        0.45060293,  0.25467651, -0.37681175,  0.71469543,  0.5421958 ,\n        0.26286192, -0.40470528,  0.72282356,  0.44963168,  0.26386903,\n       -0.3437951 ,  0.71015541,  0.5475224 ,  0.65391561, -0.0262422 ,\n        0.93800295,  0.45411116,  0.65030685,  0.02655219,  0.93829744,\n        0.58450327,  0.73805051, -0.1242262 ,  0.80984967,  0.38846929,\n        0.73128226, -0.1293727 ,  0.80878366,  0.52361624,  0.85906562,\n        0.20707419,  0.67703249,  0.48969989,  0.81531495,  0.24760007,\n        0.63339596,  0.51224869,  0.87855732,  0.23834217,  0.65351315,\n        0.50203469,  0.82330938,  0.28370605,  0.63512068,  0.51107437,\n        0.91659847,  0.09408345,  0.68097499,  0.51015989,  0.87693166,\n        0.18806153,  0.63199476]), 'warrior2': array([ 0.4859515 ,  0.30834162, -0.2162533 ,  0.93317194,  0.48961427,\n        0.2915915 , -0.21698193,  0.93996301,  0.49269612,  0.29197112,\n       -0.21701721,  0.94553684,  0.49535886,  0.29151708, -0.21715315,\n        0.942471  ,  0.48477973,  0.29130474, -0.21405157,  0.93361616,\n        0.48258888,  0.29128951, -0.21405416,  0.93314888,  0.48061992,\n        0.29146811, -0.21427933,  0.92636357,  0.50153882,  0.30371053,\n       -0.16535707,  0.94405825,  0.48069796,  0.30195649, -0.15626662,\n        0.92826917,  0.49151821,  0.32961868, -0.19386851,  0.93986105,\n        0.4846511 ,  0.32960477, -0.19032982,  0.92980106,  0.53620987,\n        0.40105149, -0.13852995,  0.96678717,  0.45189413,  0.39602382,\n       -0.11467582,  0.95780345,  0.58144161,  0.4240396 , -0.1641546 ,\n        0.76861703,  0.40945799,  0.40508474, -0.12924008,  0.75871962,\n        0.60895495,  0.40118491, -0.23356832,  0.78092232,  0.38258188,\n        0.36624661, -0.18261899,  0.77546445,  0.61720296,  0.39326839,\n       -0.26334413,  0.75717025,  0.3727013 ,  0.35782061, -0.2053497 ,\n        0.748631  ,  0.61882166,  0.38361669, -0.27998126,  0.76365672,\n        0.37110082,  0.3504044 , -0.22789037,  0.75401283,  0.61522454,\n        0.388739  , -0.24967304,  0.76330602,  0.37418693,  0.35517303,\n       -0.19690874,  0.7569329 ,  0.52497979,  0.71150407, -0.01680928,\n        0.94074688,  0.46748057,  0.71119394,  0.01686991,  0.93949194,\n        0.58159444,  0.7530479 , -0.02191832,  0.8423232 ,  0.39803318,\n        0.74277508, -0.01471572,  0.84371914,  0.59709131,  0.89523171,\n        0.20335147,  0.79454165,  0.39590226,  0.89233974,  0.17988508,\n        0.79109623,  0.59272509,  0.91398238,  0.22456218,  0.76570924,\n        0.40323601,  0.91315403,  0.20276368,  0.78351107,  0.6073125 ,\n        0.94427987,  0.11628653,  0.78247786,  0.3787371 ,  0.94170792,\n        0.10860539,  0.77314384])}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"DATASET_PATH = \"/kaggle/input/yoga-poses-dataset/DATASET/TEST\"  # Replace with your dataset folder path\nPOSE_LABELS = [\"downdog\", \"goddess\", \"plank\", \"tree\", \"warrior2\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.194248Z","iopub.status.idle":"2024-12-10T19:57:40.194538Z","shell.execute_reply.started":"2024-12-10T19:57:40.194397Z","shell.execute_reply":"2024-12-10T19:57:40.194412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for pose_label in POSE_LABELS:\n    pose_dir = os.path.join(DATASET_PATH, pose_label)\n    for img_file in os.listdir(pose_dir):\n        img_path = os.path.join(pose_dir, img_file)\n\n        # Read the image\n        image = cv2.imread(img_path)\n        if image is None:\n            print(f\"Failed to read {img_path}\")\n            continue\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Process the image with Mediapipe\n        results = pose.process(image_rgb)\n\n        if results.pose_landmarks:\n            # Extract keypoints\n            landmarks = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark]).flatten()\n            data.append([pose_label] + landmarks.tolist())\n        else:\n            print(f\"No landmarks detected for {img_file} {pose_label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.195464Z","iopub.status.idle":"2024-12-10T19:57:40.195784Z","shell.execute_reply.started":"2024-12-10T19:57:40.195609Z","shell.execute_reply":"2024-12-10T19:57:40.195624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.197381Z","iopub.status.idle":"2024-12-10T19:57:40.197672Z","shell.execute_reply.started":"2024-12-10T19:57:40.197519Z","shell.execute_reply":"2024-12-10T19:57:40.197533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nrandom.shuffle(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.198864Z","iopub.status.idle":"2024-12-10T19:57:40.199130Z","shell.execute_reply.started":"2024-12-10T19:57:40.198997Z","shell.execute_reply":"2024-12-10T19:57:40.199010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.200130Z","iopub.status.idle":"2024-12-10T19:57:40.200389Z","shell.execute_reply.started":"2024-12-10T19:57:40.200259Z","shell.execute_reply":"2024-12-10T19:57:40.200272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns = [\"label\"] + [f\"kp_{i}\" for i in range(len(data[0]) - 1)]\ndf = pd.DataFrame(data, columns=columns)\ndf.to_csv(\"/kaggle/working/yoga_pose_dataset_2.csv\", index=False)\nprint(\"Keypoints dataset saved as yoga_pose_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.201826Z","iopub.status.idle":"2024-12-10T19:57:40.202104Z","shell.execute_reply.started":"2024-12-10T19:57:40.201964Z","shell.execute_reply":"2024-12-10T19:57:40.201978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/working/yoga_pose_dataset_2.csv\")\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.202718Z","iopub.status.idle":"2024-12-10T19:57:40.203006Z","shell.execute_reply.started":"2024-12-10T19:57:40.202866Z","shell.execute_reply":"2024-12-10T19:57:40.202881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.204086Z","iopub.status.idle":"2024-12-10T19:57:40.204385Z","shell.execute_reply.started":"2024-12-10T19:57:40.204244Z","shell.execute_reply":"2024-12-10T19:57:40.204259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/working/yoga_pose_dataset_2.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.205567Z","iopub.status.idle":"2024-12-10T19:57:40.205864Z","shell.execute_reply.started":"2024-12-10T19:57:40.205725Z","shell.execute_reply":"2024-12-10T19:57:40.205739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = data.drop(\"label\", axis=1).values  # Keypoints\ny = data[\"label\"].values ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.206564Z","iopub.status.idle":"2024-12-10T19:57:40.206878Z","shell.execute_reply.started":"2024-12-10T19:57:40.206731Z","shell.execute_reply":"2024-12-10T19:57:40.206747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.207629Z","iopub.status.idle":"2024-12-10T19:57:40.207926Z","shell.execute_reply.started":"2024-12-10T19:57:40.207790Z","shell.execute_reply":"2024-12-10T19:57:40.207805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.209093Z","iopub.status.idle":"2024-12-10T19:57:40.209379Z","shell.execute_reply.started":"2024-12-10T19:57:40.209240Z","shell.execute_reply":"2024-12-10T19:57:40.209255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.210700Z","iopub.status.idle":"2024-12-10T19:57:40.211001Z","shell.execute_reply.started":"2024-12-10T19:57:40.210859Z","shell.execute_reply":"2024-12-10T19:57:40.210874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_onehot = np.eye(len(np.unique(y_encoded)))[y_encoded]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.211897Z","iopub.status.idle":"2024-12-10T19:57:40.212161Z","shell.execute_reply.started":"2024-12-10T19:57:40.212029Z","shell.execute_reply":"2024-12-10T19:57:40.212042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_onehot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.213403Z","iopub.status.idle":"2024-12-10T19:57:40.213730Z","shell.execute_reply.started":"2024-12-10T19:57:40.213558Z","shell.execute_reply":"2024-12-10T19:57:40.213573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.25, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.214732Z","iopub.status.idle":"2024-12-10T19:57:40.215004Z","shell.execute_reply.started":"2024-12-10T19:57:40.214870Z","shell.execute_reply":"2024-12-10T19:57:40.214884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Sequential([\n    Dense(128, input_dim=X_train.shape[1], activation='relu'),\n    Dropout(0.3),\n    Dense(64, activation='relu'),\n    Dropout(0.3),\n    Dense(len(y_onehot[0]), activation='softmax')  # Number of output classes\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.216862Z","iopub.status.idle":"2024-12-10T19:57:40.217295Z","shell.execute_reply.started":"2024-12-10T19:57:40.217069Z","shell.execute_reply":"2024-12-10T19:57:40.217092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.218444Z","iopub.status.idle":"2024-12-10T19:57:40.218899Z","shell.execute_reply.started":"2024-12-10T19:57:40.218664Z","shell.execute_reply":"2024-12-10T19:57:40.218687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=75, batch_size=32, validation_split=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.220481Z","iopub.status.idle":"2024-12-10T19:57:40.220934Z","shell.execute_reply.started":"2024-12-10T19:57:40.220699Z","shell.execute_reply":"2024-12-10T19:57:40.220723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.222299Z","iopub.status.idle":"2024-12-10T19:57:40.222739Z","shell.execute_reply.started":"2024-12-10T19:57:40.222499Z","shell.execute_reply":"2024-12-10T19:57:40.222521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"/kaggle/working/yoga_pose_neural_network_model.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.224019Z","iopub.status.idle":"2024-12-10T19:57:40.224442Z","shell.execute_reply.started":"2024-12-10T19:57:40.224220Z","shell.execute_reply":"2024-12-10T19:57:40.224244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\n\n# Build the enhanced model\nmodel = Sequential([\n    Dense(1024, input_dim=X_train.shape[1], activation='relu'),\n    BatchNormalization(),\n    Dropout(0.4),\n    \n    Dense(512, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.4),\n    \n    Dense(256, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.3),\n    \n    Dense(128, activation='relu'),\n    Dropout(0.3),\n    \n    Dense(len(y_onehot[0]), activation='softmax')  # Output layer with number of classes\n])\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.226064Z","iopub.status.idle":"2024-12-10T19:57:40.226489Z","shell.execute_reply.started":"2024-12-10T19:57:40.226267Z","shell.execute_reply":"2024-12-10T19:57:40.226289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.228556Z","iopub.status.idle":"2024-12-10T19:57:40.228910Z","shell.execute_reply.started":"2024-12-10T19:57:40.228756Z","shell.execute_reply":"2024-12-10T19:57:40.228772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=75, batch_size=32, validation_split=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.229925Z","iopub.status.idle":"2024-12-10T19:57:40.230216Z","shell.execute_reply.started":"2024-12-10T19:57:40.230069Z","shell.execute_reply":"2024-12-10T19:57:40.230089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.231318Z","iopub.status.idle":"2024-12-10T19:57:40.231606Z","shell.execute_reply.started":"2024-12-10T19:57:40.231466Z","shell.execute_reply":"2024-12-10T19:57:40.231481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"/kaggle/working/yoga_pose_neural_network_1024_model.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.232385Z","iopub.status.idle":"2024-12-10T19:57:40.232673Z","shell.execute_reply.started":"2024-12-10T19:57:40.232519Z","shell.execute_reply":"2024-12-10T19:57:40.232532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.233964Z","iopub.status.idle":"2024-12-10T19:57:40.234383Z","shell.execute_reply.started":"2024-12-10T19:57:40.234163Z","shell.execute_reply":"2024-12-10T19:57:40.234185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_model = xgb.XGBClassifier(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=6,\n    random_state=42,\n    use_label_encoder=False\n)\n\nxgb_model.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = xgb_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\nprint(classification_report(y_test, y_pred, target_names=label_encoder.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.235429Z","iopub.status.idle":"2024-12-10T19:57:40.235877Z","shell.execute_reply.started":"2024-12-10T19:57:40.235624Z","shell.execute_reply":"2024-12-10T19:57:40.235665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_model.save_model(\"/kaggle/working/yoga_pose_xgb_model.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.237205Z","iopub.status.idle":"2024-12-10T19:57:40.237632Z","shell.execute_reply.started":"2024-12-10T19:57:40.237412Z","shell.execute_reply":"2024-12-10T19:57:40.237434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(\n    n_estimators=100,  # Number of trees in the forest\n    max_depth=None,    # Maximum depth of the tree (None means no limit)\n    random_state=42,   # Seed for reproducibility\n    n_jobs=-1          # Use all available CPU cores\n)\nrf_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = rf_model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n\n# Classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=label_encoder.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.238827Z","iopub.status.idle":"2024-12-10T19:57:40.239240Z","shell.execute_reply.started":"2024-12-10T19:57:40.239022Z","shell.execute_reply":"2024-12-10T19:57:40.239044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n\n# Save the Random Forest model\njoblib.dump(rf_model, \"/kaggle/working/yoga_pose_rf_model.pkl\")\nprint(\"Random Forest model saved as yoga_pose_rf_model.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.240496Z","iopub.status.idle":"2024-12-10T19:57:40.240941Z","shell.execute_reply.started":"2024-12-10T19:57:40.240709Z","shell.execute_reply":"2024-12-10T19:57:40.240732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\n\n# Build the enhanced model\nmodel = Sequential([\n    Dense(1024, input_dim=X_train.shape[1], activation='relu'),\n    BatchNormalization(),\n    Dropout(0.4),\n    \n    Dense(512, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.4),\n    \n    Dense(256, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.3),\n    \n    Dense(128, activation='relu'),\n    Dropout(0.3),\n    \n    Dense(len(y_onehot[0]), activation='softmax')  # Output layer with number of classes\n])\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.242305Z","iopub.status.idle":"2024-12-10T19:57:40.242748Z","shell.execute_reply.started":"2024-12-10T19:57:40.242504Z","shell.execute_reply":"2024-12-10T19:57:40.242526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=0.0005),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.243881Z","iopub.status.idle":"2024-12-10T19:57:40.244297Z","shell.execute_reply.started":"2024-12-10T19:57:40.244076Z","shell.execute_reply":"2024-12-10T19:57:40.244098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.245714Z","iopub.status.idle":"2024-12-10T19:57:40.246148Z","shell.execute_reply.started":"2024-12-10T19:57:40.245923Z","shell.execute_reply":"2024-12-10T19:57:40.245947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.247206Z","iopub.status.idle":"2024-12-10T19:57:40.247674Z","shell.execute_reply.started":"2024-12-10T19:57:40.247420Z","shell.execute_reply":"2024-12-10T19:57:40.247443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_model = xgb.XGBClassifier(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=6,\n    random_state=42,\n    use_label_encoder=False\n)\n\nxgb_model.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = xgb_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\nprint(classification_report(y_test, y_pred, target_names=label_encoder.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.248768Z","iopub.status.idle":"2024-12-10T19:57:40.249198Z","shell.execute_reply.started":"2024-12-10T19:57:40.248979Z","shell.execute_reply":"2024-12-10T19:57:40.249002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python --version\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.250541Z","iopub.status.idle":"2024-12-10T19:57:40.250991Z","shell.execute_reply.started":"2024-12-10T19:57:40.250758Z","shell.execute_reply":"2024-12-10T19:57:40.250780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip show mediapipe\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:57:40.252708Z","iopub.status.idle":"2024-12-10T19:57:40.253142Z","shell.execute_reply.started":"2024-12-10T19:57:40.252920Z","shell.execute_reply":"2024-12-10T19:57:40.252942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}